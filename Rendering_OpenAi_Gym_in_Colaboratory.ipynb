{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rendering OpenAi Gym in Colaboratory.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IssamBenamara/SIFT_BOW/blob/master/Rendering_OpenAi_Gym_in_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "# install dependancies, takes around 45 seconds\n",
        "\n",
        "Rendering Dependancies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-1LTSH88EE",
        "colab_type": "text"
      },
      "source": [
        "Pacman Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3BGbWOu179M",
        "colab_type": "text"
      },
      "source": [
        "# Pacman!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGEFMfDOzLen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = wrap_env(gym.make(\"MsPacman-v0\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmIlXhe9Q89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check out the pacman action space!\n",
        "print(env.action_space)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nj5sjsk15IT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample() \n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "   \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc3vdeVAKAcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"TkAgg\")\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYdlX3JlKFkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip gridworld.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPpflaTxKK1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomAgent(object):\n",
        "    \"\"\"The world's simplest agent!\"\"\"\n",
        "\n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def act(self, observation, reward, done):\n",
        "        return self.action_space.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GyTaHYiKMha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NN(torch.nn.Module):\n",
        "\t\"\"\" simple Neural Net \"\"\"\n",
        "\tdef __init__(self, inSize, outSize, layers=[]):\n",
        "\t\tsuper(NN, self).__init__()\n",
        "\t\tself.layers = nn.ModuleList([])\n",
        "\t\tfor x in layers:\n",
        "\t\t\tself.layers.append(nn.Linear(inSize, x))\n",
        "\t\t\tinSize = x\n",
        "\t\tself.layers.append(nn.Linear(inSize, outSize))\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.layers[0](x)\n",
        "\t\tfor i in range(1, len(self.layers)):\n",
        "\t\t\tx = torch.nn.functional.leaky_relu(x)\n",
        "\t\t\tx = self.layers[i](x)\n",
        "\t\treturn x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wDlBdyKKRr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(object):\n",
        "    def __init__(self):\n",
        "        self.eval_net, self.target_net = NN(N_STATES, N_ACTIONS, layers=LAYERS), NN(N_STATES, N_ACTIONS, layers=LAYERS)\n",
        "\n",
        "        self.learn_step_counter = 0                                     # for target updating\n",
        "        self.memory_counter = 0                                         # for storing memory\n",
        "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory\n",
        "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
        "        self.loss_func = nn.SmoothL1Loss()\n",
        "\n",
        "    def act(self, x):\n",
        "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
        "        # input only one sample\n",
        "        if np.random.uniform() < EPSILON:   # greedy\n",
        "            actions_value = self.eval_net.forward(x)\n",
        "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
        "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
        "        else:   # random\n",
        "            action = np.random.randint(0, N_ACTIONS)\n",
        "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
        "        return action\n",
        "\n",
        "    def store_transition(self, s, a, r, s_):\n",
        "        transition = np.hstack((s, [a, r], s_))\n",
        "        # replace the old memory with new memory\n",
        "        index = self.memory_counter % MEMORY_CAPACITY\n",
        "        self.memory[index, :] = transition\n",
        "        self.memory_counter += 1\n",
        "\n",
        "    def learn(self):\n",
        "        # target parameter update\n",
        "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
        "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
        "        self.learn_step_counter += 1\n",
        "\n",
        "        # sample batch transitions\n",
        "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
        "        b_memory = self.memory[sample_index, :]\n",
        "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
        "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
        "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
        "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
        "\n",
        "        # q_eval w.r.t the action in experience\n",
        "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
        "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
        "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
        "        loss = self.loss_func(q_eval, q_target)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ud-pdiUJ-ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper Parameters\n",
        "LAYERS = [32]\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.01                   # learning rate\n",
        "EPSILON = 0.9               # greedy policy\n",
        "GAMMA = 0.9                 # reward discount\n",
        "TARGET_REPLACE_ITER = 100   # target update frequency\n",
        "MEMORY_CAPACITY = 2000\n",
        "N_ACTIONS = env.action_space.n\n",
        "N_STATES = env.observation_space.shape[0]\n",
        "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     # to confirm the shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dqn = DQN()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    env = gym.make('CartPole-v1')\n",
        "\n",
        "    # Enregistrement de l'Agent\n",
        "    dqn = DQN()\n",
        "    agent = RandomAgent(env.action_space)\n",
        "\n",
        "    outdir = 'cartpole-v0/random-agent-results'\n",
        "    envm = wrappers.Monitor(env, directory=outdir, force=True, video_callable=False)\n",
        "    env.seed(0)\n",
        "\n",
        "    episode_count = 1000000\n",
        "    reward = 0\n",
        "    done = False\n",
        "    env.verbose = True\n",
        "    np.random.seed(5)\n",
        "    rsum = 0\n",
        "\n",
        "\n",
        "    for i in range(episode_count):\n",
        "        obs = envm.reset()\n",
        "        env.verbose = (i % 100 == 0 and i > 0)  # afficher 1 episode sur 100\n",
        "        if env.verbose:\n",
        "            env.render()\n",
        "        j = 0\n",
        "        rsum = 0\n",
        "        while True:\n",
        "            action = dqn.act(obs)\n",
        "            obs_, reward, done, _ = envm.step(action)\n",
        "            #s_, r, done, info = env.step(a)\n",
        "            r = reward\n",
        "            x, x_dot, theta, theta_dot = obs_\n",
        "            r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
        "            r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
        "            r = r1 + r2\n",
        "    \n",
        "            dqn.store_transition(obs, action, r, obs_)\n",
        "            rsum += r\n",
        "            j += 1\n",
        "            \n",
        "            \n",
        "            if dqn.memory_counter > MEMORY_CAPACITY:\n",
        "                dqn.learn()\n",
        "                if done:\n",
        "                    print(\"Episode : \" + str(i) + \" rsum=\" + str(rsum) + \", \" + str(j) + \" actions\")\n",
        "    \n",
        "            \n",
        "            obs = obs_\n",
        "            \n",
        "            \n",
        "            \n",
        "            if env.verbose:\n",
        "                env.render()\n",
        "            if done:\n",
        "                print(\"Episode : \" + str(i) + \" rsum=\" + str(rsum) + \", \" + str(j) + \" actions\")\n",
        "                break\n",
        "\n",
        "    print(\"done\")\n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}